{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00db6b25",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook shows how we created the VLM fine-tuning dataset for our pipeline. It uses the data generated by runninng\n",
    "- `synthetic/query_writing.py` -> used to create query writing examples\n",
    "- `agents/data_gen_agent.py` -> used to create re-ranker and answer generation examples\n",
    "\n",
    "We used the `public_test` split of `crag-mm-2025/crag-mm-single-turn-public (v0.1.2)` dataset to create task 2  (multi-source augmentation) fine-tuning dataset. This notebook demos the dataset creation steps using a subset of examples from the above dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7767f",
   "metadata": {},
   "source": [
    "# Eval Utils\n",
    "\n",
    "LLM-as-a-judge set up for curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e91b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import Image as HFImage\n",
    "from datasets import load_dataset\n",
    "from openai import AzureOpenAI\n",
    "from pydantic import BaseModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "import concurrent.futures\n",
    "\n",
    "class CRAGTurnEvaluationResult(BaseModel):\n",
    "    \"\"\"Structured output model for CRAG turn evaluation results.\"\"\"\n",
    "    accuracy: bool\n",
    "        \n",
    "def get_system_message() -> str:\n",
    "    \"\"\"\n",
    "    Returns the system message for the evaluator.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"You are an expert evaluator for question answering systems. \"\n",
    "        \"Your task is to determine if a prediction correctly answers a question based on the ground truth.\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"1. The prediction is correct if it captures all the key information from the ground truth.\\n\"\n",
    "        \"2. The prediction is correct even if phrased differently as long as the meaning is the same.\\n\"\n",
    "        \"3. The prediction is incorrect if it contains incorrect information or is missing essential details.\\n\"\n",
    "        \"Output a JSON object with a single field 'accuracy' whose value is true or false.\"\n",
    "    )\n",
    "\n",
    "def attempt_api_call(client, model_name, messages, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(model=model_name, messages=messages, response_format=CRAGTurnEvaluationResult)\n",
    "            return completion.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            time.sleep(1)\n",
    "            error_message = f\"API call failed on attempt {attempt + 1}/{max_retries}: {str(e)}\"\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"[red]Failed after {max_retries} attempts: {str(e)}[/red]\")\n",
    "            else:\n",
    "                print(f\"[yellow]{error_message}, retrying...[/yellow]\")\n",
    "    return None\n",
    "\n",
    "def evaluate_response(query, ground_truth, agent_response, eval_model_name = \"gpt-4o\"):\n",
    "    is_idk = \"i don't know\" in agent_response.lower()\n",
    "    is_exact_match = agent_response.strip().lower() == ground_truth.strip().lower()\n",
    "    is_semantically_correct = False\n",
    "    api_response = None\n",
    "\n",
    "    is_correct = is_exact_match\n",
    "\n",
    "    if not is_idk and not is_exact_match:\n",
    "        local_openai_client = OpenAI()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": get_system_message()},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {query}\\nGround truth: {ground_truth}\\nPrediction: {agent_response}\\n\"},\n",
    "        ]\n",
    "        \n",
    "        api_response = attempt_api_call(local_openai_client, eval_model_name, messages)\n",
    "        \n",
    "        if api_response:\n",
    "            is_semantically_correct = api_response.accuracy\n",
    "            is_correct = is_semantically_correct\n",
    "    if is_exact_match:\n",
    "        is_semantically_correct = True\n",
    "\n",
    "    return {\n",
    "        \"is_exact_match\": is_exact_match,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"is_miss\": is_idk,\n",
    "        \"is_semantically_correct\": is_semantically_correct,\n",
    "        \"api_response\": api_response.model_dump() if api_response else None,\n",
    "    }\n",
    "\n",
    "def evaluate_response_parallel(responses, max_workers=8):\n",
    "    \"\"\"\n",
    "    Evaluate responses in parallel using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        responses: List of response dictionaries\n",
    "        max_workers: Number of parallel threads to use\n",
    "    \"\"\"\n",
    "    \n",
    "    def evaluate_single_response(response):\n",
    "        query = response['query']\n",
    "        ground_truth = response['answer']\n",
    "        agent_response = response['predicted_answer']\n",
    "        \n",
    "        evaluation_result = evaluate_response(query, ground_truth, agent_response)\n",
    "\n",
    "        response['eval_is_correct'] = evaluation_result['is_correct']\n",
    "        response['eval_is_semantically_correct'] = evaluation_result['is_semantically_correct']\n",
    "        response['eval_is_miss'] = evaluation_result['is_miss']\n",
    "        response[\"eval_api_response\"] = evaluation_result['api_response']\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_response = {\n",
    "            executor.submit(evaluate_single_response, response): i \n",
    "            for i, response in enumerate(responses)\n",
    "        }\n",
    "        \n",
    "        with tqdm(total=len(responses), desc=\"Evaluating responses\") as pbar:\n",
    "            for future in concurrent.futures.as_completed(future_to_response):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    pbar.update(1)\n",
    "                except Exception as exc:\n",
    "                    idx = future_to_response[future]\n",
    "                    print(f'Response {idx} generated an exception: {exc}')\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb7853",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a79cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974f4ef",
   "metadata": {},
   "source": [
    "# Read Data Agent Outputs\n",
    "\n",
    "Read the outputs generated from running `python data_gen.py --output_dir data/kddcup25_datagen_outputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4190cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a639ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed3b31fc6a94bfeaddbaa66e47c43a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1092 unique interaction_ids\n",
      "Groups with at least one correct answer: 675\n",
      "Total groups: 1092\n",
      "Percentage: 61.8%\n"
     ]
    }
   ],
   "source": [
    "dataset_dirs = []\n",
    "dataset_dirs.extend(glob.glob(\"../data/kddcup25_datagen_outputs/*/\"))\n",
    "\n",
    "ds_list = []\n",
    "for d in dataset_dirs:\n",
    "    ds_list.append(load_from_disk(d))\n",
    "\n",
    "ds = concatenate_datasets(ds_list)\n",
    "\n",
    "interaction_groups = defaultdict(list) # group interaction_id\n",
    "\n",
    "pbar = tqdm(range(len(ds)))\n",
    "for i, example in enumerate(ds):\n",
    "    interaction_id = example['interaction_id']\n",
    "    interaction_groups[interaction_id].append((i, example))\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "print(f\"Found {len(interaction_groups)} unique interaction_ids\")\n",
    "\n",
    "groups_with_correct = 0\n",
    "total_groups = len(interaction_groups)\n",
    "\n",
    "for interaction_id, examples in interaction_groups.items():\n",
    "    has_correct = any(example[1]['eval_is_correct'] for example in examples)\n",
    "    if has_correct:\n",
    "        groups_with_correct += 1\n",
    "\n",
    "print(f\"Groups with at least one correct answer: {groups_with_correct}\")\n",
    "print(f\"Total groups: {total_groups}\")\n",
    "print(f\"Percentage: {groups_with_correct/total_groups*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abee947",
   "metadata": {},
   "source": [
    "# Curation\n",
    "\n",
    "- Double check correctness of examples where context relevance is high but answer is wrong. \n",
    "- Now check with GPT-4o instead of GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6692b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_eval_examples = []\n",
    "\n",
    "double_check_relevance_th = 0.8\n",
    "\n",
    "for this_id, instances in interaction_groups.items():    \n",
    "    for _, ex in instances:\n",
    "        if ex['eval_is_correct']:\n",
    "            continue\n",
    "        if ex['context_relevance'] > double_check_relevance_th:\n",
    "            re_eval_examples.append(ex)\n",
    "            \n",
    "len(re_eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32dca83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b1858638574193b449fd2bc6ff31e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating responses:   0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_response_parallel(re_eval_examples, max_workers=8)\n",
    "sum([x['eval_is_correct'] for x in re_eval_examples]) # number of cases where correctness judgement changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ea6c2",
   "metadata": {},
   "source": [
    "# Process & filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d0d8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45c7b3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaba8de4186440eaf264db6d67f06c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rank_cutoff = 8 # don't include context beyond this rank for fine-tuning dataset\n",
    "rel_skip_th = 0.8 # don't mark a context as negative above this threshold even if it didn't lead to correct answer\n",
    "negative_ratio = 10 # maximum number of irrelevant context for re-ranker per query\n",
    "\n",
    "main_task_examples = []\n",
    "relevance_task_examples = []\n",
    "\n",
    "pbar = tqdm(range(len(interaction_groups)))\n",
    "n_contexts = []\n",
    "\n",
    "for this_id, instances in interaction_groups.items():\n",
    "    rank = 0\n",
    "    contexts = []\n",
    "    good_contexts = []\n",
    "    at_least_one_correct = False\n",
    "    rel_examples = []\n",
    "    nc = 0\n",
    "    nneg = 0\n",
    "    \n",
    "    instances = sorted(instances, key=lambda x: x[1]['context_relevance'], reverse=True)\n",
    "    good_answers = []\n",
    "    \n",
    "    for _, ex in instances:\n",
    "        contexts.append(ex['search_content'])\n",
    "        nc += 1\n",
    "        \n",
    "        if ex['eval_is_correct']:\n",
    "            at_least_one_correct = True\n",
    "            good_contexts.append(ex['search_content'])\n",
    "            good_answers.append(ex['predicted_answer'])\n",
    "\n",
    "        if ex['eval_is_correct'] or ex['context_relevance'] <= rel_skip_th:\n",
    "            rel_example = {\n",
    "                'session_id': ex['session_id'],\n",
    "                'interaction_id': ex['interaction_id'],\n",
    "                'query': ex['query'],\n",
    "                'message_history': ex['message_history'],\n",
    "                'context': ex['search_content'],\n",
    "                'is_relevant': \"Yes\" if ex['eval_is_correct'] else \"No\",\n",
    "            }\n",
    "            \n",
    "            if not ex['eval_is_correct']:\n",
    "                nneg += 1\n",
    "                if nneg <= negative_ratio:\n",
    "                    rel_examples.append(rel_example)\n",
    "            else:\n",
    "                rel_examples.append(rel_example)\n",
    "            \n",
    "        rank += 1\n",
    "        if rank == rank_cutoff:\n",
    "            break\n",
    "            \n",
    "    #---\n",
    "    if at_least_one_correct:\n",
    "        answer = ex['answer'] # all answers in a group are same, so we can use the last one to get the answer\n",
    "        if len(rel_examples) > 6:\n",
    "            rel_examples = random.sample(rel_examples, 5)\n",
    "        relevance_task_examples.extend(rel_examples)\n",
    "    else:\n",
    "        answer = \"I don't know\"\n",
    "        if len(rel_examples) > 0:\n",
    "            rel_examples = random.sample(rel_examples, 1)\n",
    "            relevance_task_examples.extend(rel_examples)\n",
    "        \n",
    "    n_contexts.append(nc)\n",
    "        \n",
    "    this_example = {\n",
    "        'session_id': ex['session_id'],\n",
    "        'interaction_id': ex['interaction_id'],\n",
    "        'turn_idx': ex['turn_idx'],\n",
    "        'query': ex['query'],\n",
    "        'message_history': ex['message_history'],\n",
    "        'answer': answer,\n",
    "        'context_list': contexts,\n",
    "        'good_context_list': good_contexts,\n",
    "        'good_answer_list': good_answers,\n",
    "    }\n",
    "    pbar.update(1)\n",
    "    \n",
    "    main_task_examples.append(this_example)\n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0ce94ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1092, 3551)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_task_examples), len(relevance_task_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2b2df69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'No': 2026, 'Yes': 1525})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([ex['is_relevant'] for ex in relevance_task_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "385584c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({True: 615, False: 477})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([ex['answer'] != \"I don't know\" for ex in main_task_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d998d53",
   "metadata": {},
   "source": [
    "# Process query writing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0bcc452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 files\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "paths = glob.glob(\"../data/kddcup25_synthetic/query_writing/*.json\")\n",
    "query_list = []\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        query_list.append(data)\n",
    "\n",
    "print(f\"Loaded {len(query_list)} files\")\n",
    "mapping = {x['interaction_id']:x['web_queries'] for x in query_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "897d5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccb0a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since crag-mm-2025/crag-mm-single-turn-public couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /lustre/fsw/portfolios/datascience/users/rabiswas/.cache/huggingface/datasets/crag-mm-2025___crag-mm-single-turn-public/default/0.0.0/e3a061380b9e5c7cab76f6001173872fe9149081 (last modified on Fri Jun 13 04:15:12 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1936"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_type = \"single-turn\"\n",
    "dataset_split = \"public_test\"\n",
    "repo_name = f\"crag-mm-2025/crag-mm-{dataset_type}-public\"\n",
    "dataset = load_dataset(repo_name, revision=\"v0.1.2\")[dataset_split]\n",
    "\n",
    "dataset = dataset.remove_columns(['image'])\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efb104d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_gen_task_examples = []\n",
    "\n",
    "for ex in dataset:\n",
    "    search_strings = []\n",
    "    \n",
    "    iid = ex['turns']['interaction_id'][0]\n",
    "    if iid not in mapping: \n",
    "        continue\n",
    "    q_all = mapping[iid]\n",
    "    \n",
    "    example = {\n",
    "        'session_id': ex['session_id'],\n",
    "        'interaction_id': ex['turns']['interaction_id'][0],\n",
    "        'query': ex['turns']['query'][0],\n",
    "        'search_queries': q_all,\n",
    "    }\n",
    "            \n",
    "    q_gen_task_examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03d023",
   "metadata": {},
   "source": [
    "# SFT Dataset\n",
    "\n",
    "Multi-task dataset for VLM finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdd7f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_examples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6e9a6",
   "metadata": {},
   "source": [
    "# Task 1: Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2813cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_SYSTEM_PROMPT = \"\"\"Answer the question factually. Instructions:\n",
    "- Be specific and avoid assumptions\n",
    "- Answer based only on what you can directly observe in the image\n",
    "- Use additional context only if it's directly relevant and increases confidence\n",
    "- If uncertain or the image doesn't contain the requested information, say \"I don't know\"\n",
    "\n",
    "Provide a direct, factual answer in 1 sentence. If unsure, just say \"I don't know\" rather than guessing.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5af19607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe84a169f66340639397eeffd92e3e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "MAX_CONTEXT = 8\n",
    "n_replace, n_found = 0, 0\n",
    "UPSAMPLE = 2\n",
    "\n",
    "pbar = tqdm(range(len(main_task_examples)))\n",
    "\n",
    "for ex in main_task_examples:\n",
    "    curr = []\n",
    "    query = ex['query']\n",
    "    message_history = ex['message_history']\n",
    "    messages = [{\"role\": \"system\", \"content\": QA_SYSTEM_PROMPT}]\n",
    "\n",
    "    if message_history:\n",
    "        messages = messages + message_history\n",
    "        \n",
    "    context_candidates = ex['context_list']\n",
    "    good_candidates = ex['good_context_list']\n",
    "    \n",
    "    final_contexts = deepcopy(context_candidates)[:MAX_CONTEXT]\n",
    "    overlap = False\n",
    "    \n",
    "    for xx in final_contexts:\n",
    "        if xx in good_candidates:\n",
    "            overlap = True\n",
    "            n_found += 1\n",
    "            break\n",
    "    \n",
    "    if not overlap and len(good_candidates) > 0:\n",
    "        n_replace += 1\n",
    "        nc = random.choice([1, 2])\n",
    "        nc = min(len(good_candidates), nc)\n",
    "        \n",
    "        pos_list = random.sample([ii for ii in range(MAX_CONTEXT)], k=nc)\n",
    "        \n",
    "        for idx, pos in enumerate(pos_list):\n",
    "            if len(final_contexts) < MAX_CONTEXT:\n",
    "                final_contexts.append(good_candidates[idx])\n",
    "            else:\n",
    "                final_contexts[pos] = good_candidates[idx]\n",
    "        \n",
    "        \n",
    "    # build context\n",
    "    context = \"\\n\\n\".join([f\"[Info {i+1}] {c}\" for i, c in enumerate(final_contexts)])\n",
    "    \n",
    "    if len(context.strip()) > 0:\n",
    "        context = f\"\\n\\nHere is some additional information that may help you answer:\\n\\n{context}\"\n",
    "        messages.append({\"role\": \"user\", \"content\": context})\n",
    "        \n",
    "    messages.append({\"role\": \"user\", \"content\": [{\"type\": \"image\"}]})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Question: {query}\"})\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    this_example = {\n",
    "        'session_id': ex['session_id'],\n",
    "        'interaction_id': ex['interaction_id'],\n",
    "        'prompt': formatted_prompt,\n",
    "        'answer': ex['answer'],\n",
    "        'task': 'qa'\n",
    "    }\n",
    "    curr.append(this_example)\n",
    "    \n",
    "    \n",
    "    for ans in ex['good_answer_list']:\n",
    "        if ans.endswith('.'):\n",
    "            this_example = {\n",
    "                'session_id': ex['session_id'],\n",
    "                'interaction_id': ex['interaction_id'],\n",
    "                'prompt': formatted_prompt,\n",
    "                'answer': ans.lower(),\n",
    "                'task': 'qa'\n",
    "            }\n",
    "            curr.append(this_example)\n",
    "            \n",
    "            if len(curr) >= UPSAMPLE:\n",
    "                break\n",
    "    \n",
    "\n",
    "    processed_examples.extend(curr)\n",
    "    pbar.update(1)\n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5003981",
   "metadata": {},
   "source": [
    "# Task 2: Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7256c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKER_SYSTEM_PROMPT = \"\"\"Determine whether the provided context contains sufficient information to answer the given question about the image.\n",
    "\n",
    "Instructions:\n",
    "- Examine both the image and the additional context provided\n",
    "- Answer \"Yes\" if the context contains enough relevant information to answer the question accurately\n",
    "- Answer \"No\" if the context is insufficient, irrelevant, or contradicts what's visible in the image\n",
    "- Focus on whether the context helps answer the specific question asked\n",
    "- Consider the context as supplementary to what you can observe in the image\n",
    "\n",
    "Respond with only \"Yes\" or \"No\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2410860e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54209148cbe045c6bb67190d83ab4606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(range(len(relevance_task_examples)))\n",
    "\n",
    "for ex in relevance_task_examples:\n",
    "    query = ex['query']\n",
    "    message_history = ex['message_history']\n",
    "    context = ex['context']\n",
    "    context = f\"\\n\\nAdditional Context:\\n\\n{context}\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": RANKER_SYSTEM_PROMPT}]\n",
    "\n",
    "    if message_history:\n",
    "        messages = messages + message_history\n",
    "        \n",
    "    messages.append({\"role\": \"user\", \"content\": context})\n",
    "    messages.append({\"role\": \"user\", \"content\": [{\"type\": \"image\"}]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Question: {query}. Does the additional context offer enough information to answer the quesiton? (Yes/No)\"})\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    this_example = {\n",
    "        'session_id': ex['session_id'],\n",
    "        'interaction_id': ex['interaction_id'],\n",
    "        'prompt': formatted_prompt,\n",
    "        'answer': ex['is_relevant'],\n",
    "        'task': 'ranker'\n",
    "    }\n",
    "\n",
    "    processed_examples.append(this_example)\n",
    "    pbar.update(1)\n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a23085",
   "metadata": {},
   "source": [
    "# Task 3: Query Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "631fbf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e238b6788674b48b2fed5b7ed28d097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final task distribution: Counter({'ranker': 3551, 'qa': 1621, 'query_gen': 16})\n",
      "Total processed examples: 5188\n"
     ]
    }
   ],
   "source": [
    "Q_GEN_SYSTEM_PROMPT = \"\"\"You will be shown an image and a question about that image. Your task is to generate 8 short, diverse and effective web search queries that a person could type into a search engine (like Google) to find information that would help answer the question.\n",
    "Each query should explore a **different** angle or approach to maximize coverage. The search engine will NOT see the image. So your search queries must be based only on what *you* can observe or infer from the image and the question — not something the search engine can figure out from the image itself.\n",
    "Do not answer the question. Do not explain anything. Just return 8 search queries as a numbered list. Be specific and concise in the queries. Focus on retrieving information that would be helpful to understand the image or answer the question.\"\"\"\n",
    "\n",
    "pbar = tqdm(range(len(q_gen_task_examples)))\n",
    "\n",
    "for ex in q_gen_task_examples:\n",
    "    query = ex['query']\n",
    "    message_history = [] # ex['message_history']\n",
    "    search_queries = ex['search_queries']\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": Q_GEN_SYSTEM_PROMPT}]\n",
    "\n",
    "    if message_history:\n",
    "        messages = messages + message_history\n",
    "        \n",
    "    messages.append({\"role\": \"user\", \"content\": [{\"type\": \"image\"}]})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nNow provide 8 useful search queries.\"})\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    this_example = {\n",
    "        'session_id': ex['session_id'],\n",
    "        'interaction_id': ex['interaction_id'],\n",
    "        'prompt': formatted_prompt,\n",
    "        'answer': search_queries,\n",
    "        'task': 'query_gen'\n",
    "    }\n",
    "\n",
    "    processed_examples.append(this_example)\n",
    "    pbar.update(1)\n",
    "    \n",
    "pbar.close()\n",
    "\n",
    "print(f\"Final task distribution: {Counter([ex['task'] for ex in processed_examples])}\")\n",
    "print(f\"Total processed examples: {len(processed_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae07c1",
   "metadata": {},
   "source": [
    "# HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b294417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0462dfe4cfdf47ac94541ddad87c632c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing examples:   0%|          | 0/5188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "from datasets import Dataset, Image as HFImage\n",
    "\n",
    "dataset_dict = {}\n",
    "for key in processed_examples[0].keys():\n",
    "    dataset_dict[key] = []\n",
    "\n",
    "for example in tqdm(processed_examples, desc=\"Processing examples\"):\n",
    "    for key, value in example.items():\n",
    "        dataset_dict[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f2e722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5188 examples in 41 batches of 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816c4eb1d79e494ab08575aa9b82eeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "from datasets import Dataset, Image as HFImage\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "batch_size = 128\n",
    "total_examples = len(processed_examples)\n",
    "num_batches = (total_examples + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"Processing {total_examples} examples in {num_batches} batches of {batch_size}\")\n",
    "\n",
    "datasets_to_concat = []\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min(start_idx + batch_size, total_examples)\n",
    "    batch_examples = processed_examples[start_idx:end_idx]\n",
    "    \n",
    "    batch_dict = {}\n",
    "    for key in batch_examples[0].keys():\n",
    "        batch_dict[key] = [example[key] for example in batch_examples]\n",
    "    \n",
    "    batch_dataset = Dataset.from_dict(batch_dict)\n",
    "    datasets_to_concat.append(batch_dataset)\n",
    "    \n",
    "    del batch_dict, batch_examples\n",
    "    gc.collect()\n",
    "\n",
    "dataset = concatenate_datasets(datasets_to_concat)\n",
    "dataset = dataset.cast_column(\"image\", HFImage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "645ef3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 5188 examples\n",
      "Task distribution: Counter({'ranker': 3551, 'qa': 1621, 'query_gen': 16})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Created dataset with {len(dataset)} examples\")\n",
    "print(f\"Task distribution: {Counter([ex['task'] for ex in processed_examples])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614de214",
   "metadata": {},
   "source": [
    "# Save & Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a805eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883f11bfc3c8425984afd622e4150b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = \"../data/kddcup/datamix_demo\"\n",
    "dataset.save_to_disk(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0d00d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Dataset https://www.kaggle.com/datasets/conjuring92/kddcup25-mix-demo ...\n",
      "Starting upload for file ../data/kddcup/datamix_demo/state.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 327/327 [00:00<00:00, 363B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ../data/kddcup/datamix_demo/state.json (327B)\n",
      "Starting upload for file ../data/kddcup/datamix_demo/data-00000-of-00001.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24.5M/24.5M [00:02<00:00, 10.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ../data/kddcup/datamix_demo/data-00000-of-00001.arrow (23MB)\n",
      "Starting upload for file ../data/kddcup/datamix_demo/dataset_info.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 506/506 [00:00<00:00, 628B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ../data/kddcup/datamix_demo/dataset_info.json (506B)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dataset instance has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/datasets/conjuring92/kddcup25-mix-demo\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "def upload_dataset(handle: str, local_dir: str):\n",
    "    kagglehub.dataset_upload(handle, local_dir)\n",
    "    \n",
    "upload_dataset(\"conjuring92/kddcup25-mix-demo\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e99796",
   "metadata": {},
   "source": [
    "# Save images\n",
    "\n",
    "save images separately for efficient processing and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "190e09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from crag_image_loader import ImageLoader\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "129afdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since crag-mm-2025/crag-mm-single-turn-public couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /lustre/fsw/portfolios/datascience/users/rabiswas/.cache/huggingface/datasets/crag-mm-2025___crag-mm-single-turn-public/default/0.0.0/e3a061380b9e5c7cab76f6001173872fe9149081 (last modified on Fri Jun 13 04:15:12 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['session_id', 'image', 'image_url', 'turns', 'answers'],\n",
       "    num_rows: 1938\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ds = load_dataset(\"crag-mm-2025/crag-mm-single-turn-public\", revision=\"v0.1.2\")[\"validation\"]\n",
    "comp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "301cf4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c28d9e26e7b40d69dbd2d161b0514d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading images:   0%|          | 0/1938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1938 images\n",
      "Total sessions: 1938\n"
     ]
    }
   ],
   "source": [
    "session_to_image = {}\n",
    "\n",
    "pbar = tqdm(comp_ds, desc=\"Loading images\")\n",
    "for example in pbar:\n",
    "    session_id = example['session_id']\n",
    "    image_url = example['image_url']\n",
    "    \n",
    "    if image_url is not None:\n",
    "        img = ImageLoader(image_url).get_image()\n",
    "        session_to_image[session_id] = img\n",
    "    else:\n",
    "        img = example['image']\n",
    "        img = img.resize((960, 1280))\n",
    "        session_to_image[session_id] = img\n",
    "        \n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(f\"Successfully loaded {sum(1 for img in session_to_image.values() if img is not None)} images\")\n",
    "print(f\"Total sessions: {len(session_to_image)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a34c39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1938 items in 20 batches of 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1acd393dee4f30b30c740644d16de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 1938 examples\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Image as HFImage, concatenate_datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "session_items = list(session_to_image.items())\n",
    "total_items = len(session_items)\n",
    "num_batches = (total_items + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"Processing {total_items} items in {num_batches} batches of {batch_size}\")\n",
    "\n",
    "datasets_to_concat = []\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min(start_idx + batch_size, total_items)\n",
    "    batch_items = session_items[start_idx:end_idx]\n",
    "    \n",
    "    batch_session_ids = [item[0] for item in batch_items]\n",
    "    batch_images = [item[1] for item in batch_items]\n",
    "    \n",
    "    batch_dataset = Dataset.from_dict({\n",
    "        'session_id': batch_session_ids,\n",
    "        'image': batch_images\n",
    "    })\n",
    "    \n",
    "    datasets_to_concat.append(batch_dataset)\n",
    "    \n",
    "    del batch_session_ids, batch_images, batch_items\n",
    "    gc.collect()\n",
    "\n",
    "image_dataset = concatenate_datasets(datasets_to_concat)\n",
    "image_dataset = image_dataset.cast_column('image', HFImage())\n",
    "\n",
    "print(f\"Created dataset with {len(image_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e619c822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925ff5bd6c8b4cd0b0d2a3d0dfdd3116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/1938 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = \"../data/kddcup/validation_image_dataset\"\n",
    "image_dataset.save_to_disk(save_dir)\n",
    "# upload_dataset(\"conjuring92/kddcup25-cup-validation-images\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76c350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790f49c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kddcup25",
   "language": "python",
   "name": "kddcup25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
